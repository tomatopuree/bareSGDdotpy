here you go sam.

howework3_adogrucu conducts stochasthic gradient descent on MNIST using a neural network with only  one layer. It uses batches and learning rate annealation.

howework6_adogrucu demonstrates a pseudo-meta learner using a 2 layer architecture, with L1 and L2 regularization. It's test set performance is demonstrated to be better than the former file's learner if correct regularization parameters are used.
