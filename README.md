hw3 conducts stochasthic gradient descent on MNIST using a neural network with only  one layer. It uses batches (stochasthic, remember?) and learning rate annealation.

hw6 is different in that it is a 2 (hidden) layer architecture, and it tries out a bunch of parameters semi automatically. It's success rate is much better if trained correctly.
