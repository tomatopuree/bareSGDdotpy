here you go sam.

barebonessgd conducts stochasthic gradient descent on MNIST using a neural network with only  one layer. It uses batches and learning rate annealation.

regularizedsgd demonstrates a pseudo-meta learner using a 2 layer architecture, with L1 and L2 regularization. It's test set performance is better that former if correct regularization parameters are used.
